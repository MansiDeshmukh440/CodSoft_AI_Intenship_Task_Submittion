{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EElhXzOy1KHm",
        "outputId": "a9d21ae7-3211-468a-b3ed-e8794ab7e87b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision transformers nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TransformerCaptioningModel(nn.Module):\n",
        "    def __init__(self, feature_dim, embed_dim, num_heads, ff_hidden_dim, num_layers, vocab_size, max_len):\n",
        "        super(TransformerCaptioningModel, self).__init__()\n",
        "        self.feature_embed = nn.Linear(feature_dim, embed_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
        "        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers, num_layers, ff_hidden_dim)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        features = self.feature_embed(features).unsqueeze(0)\n",
        "        captions = captions + self.positional_encoding[:, :captions.size(1), :]\n",
        "        out = self.transformer(features, captions)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "feature_dim = 2048\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "ff_hidden_dim = 2048\n",
        "num_layers = 6\n",
        "vocab_size = 10000  # Example vocabulary size, change accordingly\n",
        "max_len = 50\n",
        "\n",
        "# Initialize the model\n",
        "model = TransformerCaptioningModel(feature_dim, embed_dim, num_heads, ff_hidden_dim, num_layers, vocab_size, max_len)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "id": "NvRokZGz3ocf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(image_path, model, transform):\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image = transform(image).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        features = model(image)\n",
        "    features = features.view(features.size(0), -1)\n",
        "    return features\n",
        "\n",
        "# Example usage for the uploaded images\n",
        "image_path_1 = '/content/1000268201_693b08cb0e.jpg'\n",
        "image_path_2 = '/content/3637013_c675de7705.jpg'\n",
        "features_1 = extract_features(image_path_1, resnet, transform)\n",
        "features_2 = extract_features(image_path_2, resnet, transform)\n",
        "print(\"Extracted Features Shape for Image 1:\", features_1.shape)\n",
        "print(\"Extracted Features Shape for Image 2:\", features_2.shape)\n"
      ],
      "metadata": {
        "id": "HaLVXFUK8_Uk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb94a622-0039-4914-9099-48b384fb0e40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Features Shape for Image 1: torch.Size([1, 2048])\n",
            "Extracted Features Shape for Image 2: torch.Size([1, 2048])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, image_paths, captions, transform, vocab, max_len=50):\n",
        "        self.image_paths = image_paths\n",
        "        self.captions = captions\n",
        "        self.transform = transform\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        caption = self.captions[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        caption = [self.vocab.get(word, self.vocab['<unk>']) for word in caption.split()]\n",
        "        caption = [self.vocab['<start>']] + caption + [self.vocab['<end>']]\n",
        "        if len(caption) < self.max_len:\n",
        "            caption = caption + [self.vocab['<pad>']] * (self.max_len - len(caption))\n",
        "        else:\n",
        "            caption = caption[:self.max_len]\n",
        "        return image, torch.tensor(caption)\n",
        "\n",
        "# Example vocabulary and dataset\n",
        "vocab = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3, 'a': 4, 'child': 5, 'playing': 6, 'by': 7, 'the': 8, 'house': 9}  # Example vocab\n",
        "image_paths = [image_path_1, image_path_2]\n",
        "captions = ['a child playing by the house', 'a woman standing by a pond']\n",
        "\n",
        "dataset = ImageCaptionDataset(image_paths, captions, transform, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "1Uf44ZnZ9eHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "class ImageCaptionDataset(Dataset):\n",
        "    def __init__(self, image_paths, captions, transform, vocab, max_len=50):\n",
        "        self.image_paths = image_paths\n",
        "        self.captions = captions\n",
        "        self.transform = transform\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        caption = self.captions[idx]\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        caption = [self.vocab.get(word, self.vocab['<unk>']) for word in caption.split()]\n",
        "        caption = [self.vocab['<start>']] + caption + [self.vocab['<end>']]\n",
        "        if len(caption) < self.max_len:\n",
        "            caption = caption + [self.vocab['<pad>']] * (self.max_len - len(caption))\n",
        "        else:\n",
        "            caption = caption[:self.max_len]\n",
        "        return image, torch.tensor(caption)\n",
        "\n",
        "# Example vocabulary and dataset\n",
        "vocab = {'<pad>': 0, '<start>': 1, '<end>': 2, '<unk>': 3, 'a': 4, 'child': 5, 'playing': 6, 'by': 7, 'the': 8, 'house': 9}  # Example vocab\n",
        "image_paths = [image_path_1, image_path_2]\n",
        "captions = ['a child playing by the house', 'a woman standing by a pond']\n",
        "\n",
        "dataset = ImageCaptionDataset(image_paths, captions, transform, vocab)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
      ],
      "metadata": {
        "id": "azf7IHlR-xUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pp8VnnZsAAYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CaptionGenerator(nn.Module):\n",
        "    def __init__(self, feature_dim, embed_dim, vocab_size, num_layers, num_heads):\n",
        "        super(CaptionGenerator, self).__init__()\n",
        "        self.feature_embed = nn.Linear(feature_dim, embed_dim)\n",
        "        self.caption_embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, 5000, embed_dim))\n",
        "        self.transformer = nn.Transformer(embed_dim, num_heads, num_layers)\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        # Ensure features is a dense tensor\n",
        "        if features.is_sparse:\n",
        "            features = features.to_dense()\n",
        "\n",
        "        # Check the shapes of features and captions\n",
        "        print(\"Original features shape:\", features.shape)\n",
        "        print(\"Original captions shape:\", captions.shape)\n",
        "\n",
        "        features = self.feature_embed(features)\n",
        "        print(\"After embedding features shape:\", features.shape)\n",
        "\n",
        "        if features.dim() == 2:  # (batch_size, embed_dim)\n",
        "            features = features.unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
        "        elif features.dim() == 3:  # already in (batch_size, 1, embed_dim) format\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected feature dimensions after embedding: {features.dim()}\")\n",
        "\n",
        "        print(\"After unsqueeze features shape:\", features.shape)\n",
        "\n",
        "        captions = self.caption_embed(captions) + self.positional_encoding[:, :captions.size(1), :]  # (batch_size, seq_len, embed_dim)\n",
        "\n",
        "        print(\"Embedded captions shape:\", captions.shape)\n",
        "\n",
        "        # Ensure features have the correct dimensions before permuting\n",
        "        if features.dim() == 3:\n",
        "            features = features.permute(1, 0, 2)  # (1, batch_size, embed_dim)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected feature dimensions: {features.dim()}\")\n",
        "\n",
        "        # Ensure captions have the correct dimensions before permuting\n",
        "        if captions.dim() == 3:\n",
        "            captions = captions.permute(1, 0, 2)  # (seq_len, batch_size, embed_dim)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected captions dimensions: {captions.dim()}\")\n",
        "\n",
        "        print(\"Permuted features shape:\", features.shape)\n",
        "        print(\"Permuted captions shape:\", captions.shape)\n",
        "\n",
        "        out = self.transformer(features, captions)  # (seq_len, batch_size, embed_dim)\n",
        "        out = self.fc(out)  # (seq_len, batch_size, vocab_size)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Assuming feature_dim, embed_dim, vocab_size, num_layers, and num_heads are defined\n",
        "feature_dim = 2048  # Example value\n",
        "embed_dim = 512  # Example value\n",
        "vocab_size = 10000  # Example value\n",
        "num_layers = 6  # Example value\n",
        "num_heads = 8  # Example value\n",
        "model = CaptionGenerator(feature_dim, embed_dim, vocab_size, num_layers, num_heads)\n"
      ],
      "metadata": {
        "id": "RhvfoyQ4AmZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "def generate_caption(model, features, vocab, max_len=20):\n",
        "    model.eval()\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Assuming features is a single sample with shape [feature_dim]\n",
        "    features = torch.tensor(features).unsqueeze(0).to(device)  # Add batch dimension\n",
        "    print(\"Initial features shape:\", features.shape)  # Debug print\n",
        "\n",
        "    caption = [vocab['<start>']]\n",
        "    for _ in range(max_len):\n",
        "        caption_tensor = torch.tensor(caption).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(features, caption_tensor)\n",
        "\n",
        "        # Debugging: Print the shape of output\n",
        "        print(\"Output shape:\", output.shape)\n",
        "\n",
        "        # Assuming output is a tensor with shape [seq_len, batch_size, vocab_size]\n",
        "        max_index = output[-1].argmax(dim=-1)  # Get the index of maximum value from the last timestep\n",
        "        next_word = max_index.item()  # Extract the scalar value of the index\n",
        "\n",
        "        # Check if the next_word index is in the vocabulary\n",
        "        if next_word not in vocab.values():\n",
        "            print(f\"Warning: Generated index {next_word} not in vocabulary\")\n",
        "            break\n",
        "\n",
        "        caption.append(next_word)\n",
        "        if next_word == vocab['<end>']:\n",
        "            break\n",
        "\n",
        "    words = [list(vocab.keys())[list(vocab.values()).index(idx)] for idx in caption if idx in vocab.values()]\n",
        "    return ' '.join(words[1:-1])  # Skip <start> and <end> tokens\n",
        "\n",
        "# Example usage\n",
        "# Assuming model, features_1, features_2, and vocab are defined elsewhere\n",
        "generated_caption_1 = generate_caption(model, features_1, vocab)\n",
        "generated_caption_2 = generate_caption(model, features_2, vocab)\n",
        "\n",
        "print(\"Generated Caption for Image 1:\", generated_caption_1)\n",
        "print(\"Generated Caption for Image 2:\", generated_caption_2)\n"
      ],
      "metadata": {
        "id": "GDD8xJENBxHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d597b04c-aa22-45f6-f54a-3f30c4906b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial features shape: torch.Size([1, 1, 2048])\n",
            "Original features shape: torch.Size([1, 1, 2048])\n",
            "Original captions shape: torch.Size([1, 1])\n",
            "After embedding features shape: torch.Size([1, 1, 512])\n",
            "After unsqueeze features shape: torch.Size([1, 1, 512])\n",
            "Embedded captions shape: torch.Size([1, 1, 512])\n",
            "Permuted features shape: torch.Size([1, 1, 512])\n",
            "Permuted captions shape: torch.Size([1, 1, 512])\n",
            "Output shape: torch.Size([1, 1, 10000])\n",
            "Warning: Generated index 5867 not in vocabulary\n",
            "Initial features shape: torch.Size([1, 1, 2048])\n",
            "Original features shape: torch.Size([1, 1, 2048])\n",
            "Original captions shape: torch.Size([1, 1])\n",
            "After embedding features shape: torch.Size([1, 1, 512])\n",
            "After unsqueeze features shape: torch.Size([1, 1, 512])\n",
            "Embedded captions shape: torch.Size([1, 1, 512])\n",
            "Permuted features shape: torch.Size([1, 1, 512])\n",
            "Permuted captions shape: torch.Size([1, 1, 512])\n",
            "Output shape: torch.Size([1, 1, 10000])\n",
            "Warning: Generated index 4633 not in vocabulary\n",
            "Generated Caption for Image 1: \n",
            "Generated Caption for Image 2: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-89-830f0e7e257c>:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  features = torch.tensor(features).unsqueeze(0).to(device)  # Add batch dimension\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9IkNxaYyC_WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZA2cNUcyCJs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "79Qz_bpOCMKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AY80iVDWCPvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIaF2iIaCTo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TpNMEqMdCXY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tp30s0EQCuvS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}